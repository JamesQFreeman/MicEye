{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Record radiologists' eye gaze when they are labeling images. Why use MicEye Versatile As a label tool, we support class label (keyboard typing) bounding boxes (mouse draw) keypoint label (use eye gaze) During aforemention annotation process, all eye movement will be recorded. Cheaper Start your research using less than 200 bucks! Comparing to other existing software, we use cheaper and more easily obtainable hardware, i.e., Tobii Eye Tracker 4C and Tobii Eye Tracker 5. Easier Here we domonstrate some use cases, you can use it on the go with a laptop or use it on your desk monitor. It should be handful if you want to take it with you to collect data. Credit to Tobii's consumer product lineup, they did pack some really sick hardware in a small form factor. And the code is easy to modify since it is PyQt. You can try it for fun! I understand there are some better eye tracking software out there. However, they are too expensive and hard to use for someone who are trying for fun. And I do believe trying for fun is great, that's why I spent time to organize the code, add comments and docs even I know more thorough literature can be found. At beginning it was not a very serious idea during the pandemic lockdown to combine eye-tracking and medical image analysis (which is my PhD project). After some google search, I found the Tobii Eye-tracker are surprisingly affordable, so I bought it with my own money (my prof paid it soon after). There're no easy-to-use API so at the beginning I record the screen with a red bubble and post-process the video. And we started with the showing cat/dog using PPT so every one in the lab are interesting to take part in. Then I wrap the c++ sdk in python and start to writing the program which become the MicEye now. Linus Torvalds said he built Linux \"just for fun\". Hopefully, if you find this eye tracking things fun, MicEye could help you start something big. A demo We domonstrate an example of radiologist reading knee X-Ray images. Of cause, you could use it in natural image. This red bubble is for demonstration, do not exist in real software. In the future There will be a 3D version. There will be a version that support zoom and drag (coming very soon). Contact me Github issue or wsheng@sjtu.edu.cn","title":"Getting started"},{"location":"#_1","text":"Record radiologists' eye gaze when they are labeling images.","title":""},{"location":"#why-use-miceye","text":"","title":"Why use MicEye"},{"location":"#versatile","text":"As a label tool, we support class label (keyboard typing) bounding boxes (mouse draw) keypoint label (use eye gaze) During aforemention annotation process, all eye movement will be recorded.","title":"Versatile"},{"location":"#cheaper","text":"Start your research using less than 200 bucks! Comparing to other existing software, we use cheaper and more easily obtainable hardware, i.e., Tobii Eye Tracker 4C and Tobii Eye Tracker 5.","title":"Cheaper"},{"location":"#easier","text":"Here we domonstrate some use cases, you can use it on the go with a laptop or use it on your desk monitor. It should be handful if you want to take it with you to collect data. Credit to Tobii's consumer product lineup, they did pack some really sick hardware in a small form factor. And the code is easy to modify since it is PyQt.","title":"Easier"},{"location":"#you-can-try-it-for-fun","text":"I understand there are some better eye tracking software out there. However, they are too expensive and hard to use for someone who are trying for fun. And I do believe trying for fun is great, that's why I spent time to organize the code, add comments and docs even I know more thorough literature can be found. At beginning it was not a very serious idea during the pandemic lockdown to combine eye-tracking and medical image analysis (which is my PhD project). After some google search, I found the Tobii Eye-tracker are surprisingly affordable, so I bought it with my own money (my prof paid it soon after). There're no easy-to-use API so at the beginning I record the screen with a red bubble and post-process the video. And we started with the showing cat/dog using PPT so every one in the lab are interesting to take part in. Then I wrap the c++ sdk in python and start to writing the program which become the MicEye now. Linus Torvalds said he built Linux \"just for fun\". Hopefully, if you find this eye tracking things fun, MicEye could help you start something big.","title":"You can try it for fun!"},{"location":"#a-demo","text":"We domonstrate an example of radiologist reading knee X-Ray images. Of cause, you could use it in natural image. This red bubble is for demonstration, do not exist in real software.","title":"A demo"},{"location":"#in-the-future","text":"There will be a 3D version. There will be a version that support zoom and drag (coming very soon).","title":"In the future"},{"location":"#contact-me","text":"Github issue or wsheng@sjtu.edu.cn","title":"Contact me"},{"location":"about/","text":"Acknowledgement This software is developed by Wang Sheng at Shanghai Jiao Tong University & ShanghaiTech University, supported by Xi Ouyang, Qian Wang and Dinggang Shen. Feel free to check out our labs' websites SJTU MIC Lab and IDEA Lab ShanghaiTech . Thanks for the help from Qingyuan Zhen and Tianming Liu (Univerisity of Georgia), they help revise the paper. And many thanks to the participated radiologist volunteers: Jingyu Zhong and Liping Si. If you find this work interesting, check out this project by Karargyris et al. as well: CXR Eye Gaze .","title":"About"},{"location":"about/#acknowledgement","text":"This software is developed by Wang Sheng at Shanghai Jiao Tong University & ShanghaiTech University, supported by Xi Ouyang, Qian Wang and Dinggang Shen. Feel free to check out our labs' websites SJTU MIC Lab and IDEA Lab ShanghaiTech . Thanks for the help from Qingyuan Zhen and Tianming Liu (Univerisity of Georgia), they help revise the paper. And many thanks to the participated radiologist volunteers: Jingyu Zhong and Liping Si. If you find this work interesting, check out this project by Karargyris et al. as well: CXR Eye Gaze .","title":"Acknowledgement"},{"location":"contrastive_learning_example/","text":"Gaze in self-supervised learning In the last work Medical image supervised learning , we use gaze to help computer aided diagnosis. However, CAM are usually used in supervised learning (with annotation). In many cases, there are a lot of images that we have corresponding gaze but can not get the annotation. We proposed FocusContrast : import torch from PIL import Image import numpy as np import cv2 as cv from torchvision import transforms from typing import Callable, List, Tuple CONFIG_ZOOMIN = 2 CONFIG_DEGREE = 45 CONFIG_CUTOUT = 48 def decouple_to_numpy(pil_img: Image) -> np.array: \"\"\" This function returns the img and the saliency which are all SINGLE channel grayscale Args: pil_img ([type]): [special made for openselfsup] Returns: np.array: [img, 0-1 float], np.array: [img, 0-1 np.float32] \"\"\" np_input = np.array(pil_img, dtype=np.uint8) np_img = np_input[:, :, 0]/255 np_saliency = np_input[:, :, 1]/255 return np_img.astype(np.float32), np_saliency.astype(np.float32) def prob_wrapper(transform_function: Callable, p: float = 1.0) -> Callable: def prob_transform_func(img: np.array, saliency: np.array, *args, **kwargs): rand_01 = np.random.random() if rand_01 < p: img, saliency = transform_function(img, saliency, *args, **kwargs) return img, saliency return prob_transform_func def compose(list_of_transforms: list) -> Callable: \"\"\" returns composed transform function which have a special input image of three channel: [grayscale_org_img, saliency, zeros] Args: list_of_transforms (list): [list of transform in Operator and FocusContrastOperator] Returns: function: [composed transform, input is a pil image] \"\"\" def composed_transforms(img, saliency): for transform_func in list_of_transforms: img, saliency = transform_func(img, saliency) return img, saliency return composed_transforms def pipeline(transforms_and_probs: List[Tuple[Callable, float]]) -> Callable: \"\"\" Returns the entire process pipeline function that have a input of pil image and have a output of tensor Args: transforms_and_probs (List[Callable, float]): [()] Returns: Callable: [description] \"\"\" prob_transform_func = [] for transform_func, p in transforms_and_probs: prob_transform_func.append(prob_wrapper(transform_func, p)) composed_func = compose(prob_transform_func) def inner(pil_img: Image) -> torch.tensor: img, saliency = decouple_to_numpy(pil_img) img, saliency = composed_func(img, saliency) return torch.from_numpy(img.astype(np.float32)) return inner def torch_function_for_numpy(torch_func: Callable) -> Callable: \"\"\" This function turns a torch.tensor->torch.tensor function to np.array->np.array function Args: torch_func (function): [torch.tensor->torch.tensor] Raises: TypeError: [only support 0.0-1.0 image] Returns: function: [np.array->np.array] \"\"\" ''' This function turns a torch.tensor->torch.tensor function to np.array->array function ''' def numpy_func(np_img: np.array): assert (np_img.dtype == np.float32 and np.max(np_img) <= 1 and np.min(np_img) >= 0), \"image has to be 0-1 and np.float32\" torch_img = torch.from_numpy(np_img) torch_result = torch_func(torch_img) np_result = torch_result.numpy() return np_result return numpy_func def same_random_transform_on_both(img: np.array, saliency: np.array, np_random_transform: Callable) -> np.array: \"\"\" Perform transform function (np.array->np.array) on two np image (both 0-1 and 0-255 are supported) Args: img (np.array): [0-1 np.array] saliency (np.array): [0-1 np.array] np_random_transform (function): [np.array->np.array] Returns: np.array: [transformed img and saliency] \"\"\" w, h = img.shape coupled_input = np.zeros(shape=(3, w, h), dtype=np.float32) coupled_input[0, :, :] = img coupled_input[1, :, :] = saliency result = np_random_transform(coupled_input) result_img = result[0] result_saliency = result[1] return result_img, result_saliency class Operator: \"\"\" Augmentation operators that are not work with saliency or any other information \"\"\" # config = {'degree':} @ staticmethod def color_distort(img: np.array, saliency: np.array) -> np.array: torch_color_distort = transforms.ColorJitter( brightness=0.2, contrast=0.8) numpy_color_distort = torch_function_for_numpy(torch_color_distort) # we need a three channel channel first img = np.array([img, img, img]) result = numpy_color_distort(img) return result[0], saliency @ staticmethod def random_flip(img, saliency) -> np.array: torch_random_flip = transforms.RandomHorizontalFlip() numpy_random_flip = torch_function_for_numpy(torch_random_flip) result_img, result_saliency = same_random_transform_on_both( img, saliency, numpy_random_flip) return result_img, result_saliency @ staticmethod def random_rotate(img, saliency) -> np.array: torch_random_rotate = transforms.RandomRotation(degrees=CONFIG_DEGREE) numpy_random_rotate = torch_function_for_numpy(torch_random_rotate) result_img, result_saliency = same_random_transform_on_both( img, saliency, numpy_random_rotate) return result_img, result_saliency @ staticmethod def random_crop(img, saliency) -> np.array: zoom_in_ratio = (CONFIG_ZOOMIN-1.2) * np.random.rand(1) + 1.2 w, h = img.shape new_w, new_h = int(w*zoom_in_ratio), int(h*zoom_in_ratio) new_img = cv.resize(img, dsize=(new_w, new_h)) new_saliency = cv.resize(saliency, dsize=(new_w, new_h)) p = (np.random.randint(low=0, high=new_w-w), np.random.randint(low=0, high=new_h-h)) cropped_img = new_img[p[0]:p[0]+w, p[1]:p[1]+h] cropped_saliency = new_saliency[p[0]:p[0]+w, p[1]:p[1]+h] return cropped_img, cropped_saliency @ staticmethod def random_cutout(img, saliency, minimal_size=32) -> np.array: minimal_size = CONFIG_CUTOUT w, h = img.shape canvas = np.zeros(img.shape) pt1 = (np.random.randint(low=0, high=w-minimal_size), np.random.randint(low=0, high=h-minimal_size)) # pt2 = (pt1[0]+minimal_size, pt1[1]+minimal_size) pt2 = (np.random.randint(low=pt1[0]+minimal_size, high=w), np.random.randint(low=pt1[1]+minimal_size, high=h)) mask = cv.rectangle(canvas, pt1, pt2, color=1.0, thickness=cv.FILLED) result = np.multiply(1-mask, img) return result, saliency @ staticmethod def reshape(img, saliency, target_shape=(224,224)) -> np.array: new_img = cv.resize(img, dsize=target_shape) new_saliency = cv.resize(saliency, dsize=target_shape) return new_img, new_saliency @ staticmethod def to_RGB(img, saliency) -> np.array: if len(img.shape)==2: img = np.stack([img,img,img]) if len(saliency.shape)==2: saliency = np.stack([saliency,saliency,saliency]) return img, saliency class FocusContrastOperator: \"\"\" Augmentation operator with focus/saliency \"\"\" @ staticmethod def focus_drop(img, saliency, drop_to=0.1) -> np.array: # make this 0-1 saliency = cv.GaussianBlur(saliency, (199, 199), 0) saliency = saliency/(0.01+np.max(saliency)) # add non-saliece-value to make the image not entirely black saliency += drop_to saliency = np.clip(saliency, 0, 1) result = np.multiply(img, saliency) return result, saliency @ staticmethod def focus_crop(img, saliency, threshold=0.9, zoom_in_ratio=CONFIG_ZOOMIN) -> np.array: w, h = img.shape zoom_in_ratio = (zoom_in_ratio-1) * np.random.rand(1) + 1 # 1-zoom_in_ratio new_w, new_h = int(w*zoom_in_ratio), int(h*zoom_in_ratio) new_img = cv.resize(img, dsize=(new_w, new_h)) new_saliency = cv.resize(saliency, dsize=( new_w, new_h), interpolation=cv.INTER_NEAREST) def get_new_pt1(): pt1 = (np.random.randint(low=0, high=new_w-w), np.random.randint(low=0, high=new_h-h)) return pt1 rand_pt1 = get_new_pt1() saliency_crop = new_saliency[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] saliency_ratio = np.sum(saliency_crop) / \\ (np.sum(saliency)+1e-4)/(zoom_in_ratio**2) # if the generated crop have too little overlap with the saliency, desprecate it # and generate a new mask until it passes. counter = 0 while(saliency_ratio < threshold and counter < 200): rand_pt1 = get_new_pt1() saliency_crop = new_saliency[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] saliency_ratio = np.sum(saliency_crop) / \\ (np.sum(saliency)+1e-4)/(zoom_in_ratio**2) # print(saliency_ratio) counter += 1 cropped_img = new_img[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] cropped_saliency = new_saliency[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] return cropped_img, cropped_saliency @ staticmethod def focus_cutout(img, saliency, threshold=400, minimal_size=CONFIG_CUTOUT) -> np.array: w, h = img.shape def get_new_cutoutmask(): canvas = np.zeros(img.shape) pt1 = (np.random.randint(low=0, high=w-minimal_size), np.random.randint(low=0, high=h-minimal_size)) # pt2 = (pt1[0]+minimal_size, pt1[1]+minimal_size) pt2 = (np.random.randint(low=pt1[0]+minimal_size, high=w), np.random.randint(low=pt1[1]+minimal_size, high=h)) mask = cv.rectangle(canvas, pt1, pt2, color=1.0, thickness=cv.FILLED) return mask rand_mask = get_new_cutoutmask() # if the generated mask have too much overlap with the saliency, desprecate it # and generate a new mask until it passes. overlap = np.sum(rand_mask*saliency) # print(\"overlap is:\", overlap) counter = 0 while(overlap > threshold and counter < 100): rand_mask = get_new_cutoutmask() overlap = np.sum(rand_mask*saliency) counter += 1 result = np.multiply(1-rand_mask, img) return result, saliency","title":"Contrastive learning"},{"location":"contrastive_learning_example/#gaze-in-self-supervised-learning","text":"In the last work Medical image supervised learning , we use gaze to help computer aided diagnosis. However, CAM are usually used in supervised learning (with annotation). In many cases, there are a lot of images that we have corresponding gaze but can not get the annotation. We proposed FocusContrast : import torch from PIL import Image import numpy as np import cv2 as cv from torchvision import transforms from typing import Callable, List, Tuple CONFIG_ZOOMIN = 2 CONFIG_DEGREE = 45 CONFIG_CUTOUT = 48 def decouple_to_numpy(pil_img: Image) -> np.array: \"\"\" This function returns the img and the saliency which are all SINGLE channel grayscale Args: pil_img ([type]): [special made for openselfsup] Returns: np.array: [img, 0-1 float], np.array: [img, 0-1 np.float32] \"\"\" np_input = np.array(pil_img, dtype=np.uint8) np_img = np_input[:, :, 0]/255 np_saliency = np_input[:, :, 1]/255 return np_img.astype(np.float32), np_saliency.astype(np.float32) def prob_wrapper(transform_function: Callable, p: float = 1.0) -> Callable: def prob_transform_func(img: np.array, saliency: np.array, *args, **kwargs): rand_01 = np.random.random() if rand_01 < p: img, saliency = transform_function(img, saliency, *args, **kwargs) return img, saliency return prob_transform_func def compose(list_of_transforms: list) -> Callable: \"\"\" returns composed transform function which have a special input image of three channel: [grayscale_org_img, saliency, zeros] Args: list_of_transforms (list): [list of transform in Operator and FocusContrastOperator] Returns: function: [composed transform, input is a pil image] \"\"\" def composed_transforms(img, saliency): for transform_func in list_of_transforms: img, saliency = transform_func(img, saliency) return img, saliency return composed_transforms def pipeline(transforms_and_probs: List[Tuple[Callable, float]]) -> Callable: \"\"\" Returns the entire process pipeline function that have a input of pil image and have a output of tensor Args: transforms_and_probs (List[Callable, float]): [()] Returns: Callable: [description] \"\"\" prob_transform_func = [] for transform_func, p in transforms_and_probs: prob_transform_func.append(prob_wrapper(transform_func, p)) composed_func = compose(prob_transform_func) def inner(pil_img: Image) -> torch.tensor: img, saliency = decouple_to_numpy(pil_img) img, saliency = composed_func(img, saliency) return torch.from_numpy(img.astype(np.float32)) return inner def torch_function_for_numpy(torch_func: Callable) -> Callable: \"\"\" This function turns a torch.tensor->torch.tensor function to np.array->np.array function Args: torch_func (function): [torch.tensor->torch.tensor] Raises: TypeError: [only support 0.0-1.0 image] Returns: function: [np.array->np.array] \"\"\" ''' This function turns a torch.tensor->torch.tensor function to np.array->array function ''' def numpy_func(np_img: np.array): assert (np_img.dtype == np.float32 and np.max(np_img) <= 1 and np.min(np_img) >= 0), \"image has to be 0-1 and np.float32\" torch_img = torch.from_numpy(np_img) torch_result = torch_func(torch_img) np_result = torch_result.numpy() return np_result return numpy_func def same_random_transform_on_both(img: np.array, saliency: np.array, np_random_transform: Callable) -> np.array: \"\"\" Perform transform function (np.array->np.array) on two np image (both 0-1 and 0-255 are supported) Args: img (np.array): [0-1 np.array] saliency (np.array): [0-1 np.array] np_random_transform (function): [np.array->np.array] Returns: np.array: [transformed img and saliency] \"\"\" w, h = img.shape coupled_input = np.zeros(shape=(3, w, h), dtype=np.float32) coupled_input[0, :, :] = img coupled_input[1, :, :] = saliency result = np_random_transform(coupled_input) result_img = result[0] result_saliency = result[1] return result_img, result_saliency class Operator: \"\"\" Augmentation operators that are not work with saliency or any other information \"\"\" # config = {'degree':} @ staticmethod def color_distort(img: np.array, saliency: np.array) -> np.array: torch_color_distort = transforms.ColorJitter( brightness=0.2, contrast=0.8) numpy_color_distort = torch_function_for_numpy(torch_color_distort) # we need a three channel channel first img = np.array([img, img, img]) result = numpy_color_distort(img) return result[0], saliency @ staticmethod def random_flip(img, saliency) -> np.array: torch_random_flip = transforms.RandomHorizontalFlip() numpy_random_flip = torch_function_for_numpy(torch_random_flip) result_img, result_saliency = same_random_transform_on_both( img, saliency, numpy_random_flip) return result_img, result_saliency @ staticmethod def random_rotate(img, saliency) -> np.array: torch_random_rotate = transforms.RandomRotation(degrees=CONFIG_DEGREE) numpy_random_rotate = torch_function_for_numpy(torch_random_rotate) result_img, result_saliency = same_random_transform_on_both( img, saliency, numpy_random_rotate) return result_img, result_saliency @ staticmethod def random_crop(img, saliency) -> np.array: zoom_in_ratio = (CONFIG_ZOOMIN-1.2) * np.random.rand(1) + 1.2 w, h = img.shape new_w, new_h = int(w*zoom_in_ratio), int(h*zoom_in_ratio) new_img = cv.resize(img, dsize=(new_w, new_h)) new_saliency = cv.resize(saliency, dsize=(new_w, new_h)) p = (np.random.randint(low=0, high=new_w-w), np.random.randint(low=0, high=new_h-h)) cropped_img = new_img[p[0]:p[0]+w, p[1]:p[1]+h] cropped_saliency = new_saliency[p[0]:p[0]+w, p[1]:p[1]+h] return cropped_img, cropped_saliency @ staticmethod def random_cutout(img, saliency, minimal_size=32) -> np.array: minimal_size = CONFIG_CUTOUT w, h = img.shape canvas = np.zeros(img.shape) pt1 = (np.random.randint(low=0, high=w-minimal_size), np.random.randint(low=0, high=h-minimal_size)) # pt2 = (pt1[0]+minimal_size, pt1[1]+minimal_size) pt2 = (np.random.randint(low=pt1[0]+minimal_size, high=w), np.random.randint(low=pt1[1]+minimal_size, high=h)) mask = cv.rectangle(canvas, pt1, pt2, color=1.0, thickness=cv.FILLED) result = np.multiply(1-mask, img) return result, saliency @ staticmethod def reshape(img, saliency, target_shape=(224,224)) -> np.array: new_img = cv.resize(img, dsize=target_shape) new_saliency = cv.resize(saliency, dsize=target_shape) return new_img, new_saliency @ staticmethod def to_RGB(img, saliency) -> np.array: if len(img.shape)==2: img = np.stack([img,img,img]) if len(saliency.shape)==2: saliency = np.stack([saliency,saliency,saliency]) return img, saliency class FocusContrastOperator: \"\"\" Augmentation operator with focus/saliency \"\"\" @ staticmethod def focus_drop(img, saliency, drop_to=0.1) -> np.array: # make this 0-1 saliency = cv.GaussianBlur(saliency, (199, 199), 0) saliency = saliency/(0.01+np.max(saliency)) # add non-saliece-value to make the image not entirely black saliency += drop_to saliency = np.clip(saliency, 0, 1) result = np.multiply(img, saliency) return result, saliency @ staticmethod def focus_crop(img, saliency, threshold=0.9, zoom_in_ratio=CONFIG_ZOOMIN) -> np.array: w, h = img.shape zoom_in_ratio = (zoom_in_ratio-1) * np.random.rand(1) + 1 # 1-zoom_in_ratio new_w, new_h = int(w*zoom_in_ratio), int(h*zoom_in_ratio) new_img = cv.resize(img, dsize=(new_w, new_h)) new_saliency = cv.resize(saliency, dsize=( new_w, new_h), interpolation=cv.INTER_NEAREST) def get_new_pt1(): pt1 = (np.random.randint(low=0, high=new_w-w), np.random.randint(low=0, high=new_h-h)) return pt1 rand_pt1 = get_new_pt1() saliency_crop = new_saliency[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] saliency_ratio = np.sum(saliency_crop) / \\ (np.sum(saliency)+1e-4)/(zoom_in_ratio**2) # if the generated crop have too little overlap with the saliency, desprecate it # and generate a new mask until it passes. counter = 0 while(saliency_ratio < threshold and counter < 200): rand_pt1 = get_new_pt1() saliency_crop = new_saliency[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] saliency_ratio = np.sum(saliency_crop) / \\ (np.sum(saliency)+1e-4)/(zoom_in_ratio**2) # print(saliency_ratio) counter += 1 cropped_img = new_img[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] cropped_saliency = new_saliency[rand_pt1[0]:rand_pt1[0]+w, rand_pt1[1]:rand_pt1[1]+h] return cropped_img, cropped_saliency @ staticmethod def focus_cutout(img, saliency, threshold=400, minimal_size=CONFIG_CUTOUT) -> np.array: w, h = img.shape def get_new_cutoutmask(): canvas = np.zeros(img.shape) pt1 = (np.random.randint(low=0, high=w-minimal_size), np.random.randint(low=0, high=h-minimal_size)) # pt2 = (pt1[0]+minimal_size, pt1[1]+minimal_size) pt2 = (np.random.randint(low=pt1[0]+minimal_size, high=w), np.random.randint(low=pt1[1]+minimal_size, high=h)) mask = cv.rectangle(canvas, pt1, pt2, color=1.0, thickness=cv.FILLED) return mask rand_mask = get_new_cutoutmask() # if the generated mask have too much overlap with the saliency, desprecate it # and generate a new mask until it passes. overlap = np.sum(rand_mask*saliency) # print(\"overlap is:\", overlap) counter = 0 while(overlap > threshold and counter < 100): rand_mask = get_new_cutoutmask() overlap = np.sum(rand_mask*saliency) counter += 1 result = np.multiply(1-rand_mask, img) return result, saliency","title":"Gaze in self-supervised learning"},{"location":"data_collection/","text":"Collect Data Calibrate Use Tobii's software to do the calibration. I did try to write my own calibration but then I found Tobii's is great which even had a game to play (Now I can't find the game, seems like it is deleted). After the calibration, you can open MicEye by python miceye.py Start collection Place all the images in a same folder, then change the \"image folder\" option in config.json . The data collection begin with typing the name down. And a message is shown to the volunteers to tell them how the experiment is setted up. You can modify the message at main.py . Simply look Just press the \"Enter\" for the next image. Gaze will be recorded. Type to label This is for quantification/classification. For example, in knee X-Ray image, there is a quantification criteria named KL-Grade, range from 0 to 4, 0 means healthy and 4 is very ill. If do cat/dog classification, type 0 for cat and 1 for dog. It's on you. Draw bounding boxes You can add bounding boxes by click \"Add Bounding Boxes\", then draw some boxes. Laser eye mode It is a keypoint label, look at where you want label, and press \"L\" key. Save data The gaze data is saved automatically after the experiment is finished. In default setting, the data will be saved to logs/name year-month-day-hour-minute.csv . In the save file include multiple lines, each line include information seperated by semicolon, such as: some-dir/9063823L.png;1;[[206, 412], [206, 412], [205, 411], ..., [105, 501]];[];(-1, -1) It is orgnized by image-location;class-label;[gaze-loc-1,gaze-loc-2,...,gaze-loc-n];[bbox_x1,bbox_y1,bbox_x2,bbox_y2];keypoint-loc in which all the coordenate is the image coordinate system. For example, upper left of the image is [0,0] . Config We also provide config.json , so you can modify it. { \"image folder\": \"image folder\", \"save log to\": \"./logs\", \"random display order\": true, \"image height\": 900, \"loading wait\": 3, \"font\": \"Helvetica\", \"dark mode\": true, \"insta review\": false, \"guide mode\": true } Other options should be pretty straight forward. Let me explain \"insta review\" and \"guide mode\". \"insta review\" let you review the visualization of gaze when viewing the last image. \"guide mode\" display the annotation (if any) then display the image, this mode is for teaching which we are still working on.","title":"Data collection"},{"location":"data_collection/#collect-data","text":"","title":"Collect Data"},{"location":"data_collection/#calibrate","text":"Use Tobii's software to do the calibration. I did try to write my own calibration but then I found Tobii's is great which even had a game to play (Now I can't find the game, seems like it is deleted). After the calibration, you can open MicEye by python miceye.py","title":"Calibrate"},{"location":"data_collection/#start-collection","text":"Place all the images in a same folder, then change the \"image folder\" option in config.json . The data collection begin with typing the name down. And a message is shown to the volunteers to tell them how the experiment is setted up. You can modify the message at main.py .","title":"Start collection"},{"location":"data_collection/#simply-look","text":"Just press the \"Enter\" for the next image. Gaze will be recorded.","title":"Simply look"},{"location":"data_collection/#type-to-label","text":"This is for quantification/classification. For example, in knee X-Ray image, there is a quantification criteria named KL-Grade, range from 0 to 4, 0 means healthy and 4 is very ill. If do cat/dog classification, type 0 for cat and 1 for dog. It's on you.","title":"Type to label"},{"location":"data_collection/#draw-bounding-boxes","text":"You can add bounding boxes by click \"Add Bounding Boxes\", then draw some boxes.","title":"Draw bounding boxes"},{"location":"data_collection/#laser-eye-mode","text":"It is a keypoint label, look at where you want label, and press \"L\" key.","title":"Laser eye mode"},{"location":"data_collection/#save-data","text":"The gaze data is saved automatically after the experiment is finished. In default setting, the data will be saved to logs/name year-month-day-hour-minute.csv . In the save file include multiple lines, each line include information seperated by semicolon, such as: some-dir/9063823L.png;1;[[206, 412], [206, 412], [205, 411], ..., [105, 501]];[];(-1, -1) It is orgnized by image-location;class-label;[gaze-loc-1,gaze-loc-2,...,gaze-loc-n];[bbox_x1,bbox_y1,bbox_x2,bbox_y2];keypoint-loc in which all the coordenate is the image coordinate system. For example, upper left of the image is [0,0] .","title":"Save data"},{"location":"data_collection/#config","text":"We also provide config.json , so you can modify it. { \"image folder\": \"image folder\", \"save log to\": \"./logs\", \"random display order\": true, \"image height\": 900, \"loading wait\": 3, \"font\": \"Helvetica\", \"dark mode\": true, \"insta review\": false, \"guide mode\": true } Other options should be pretty straight forward. Let me explain \"insta review\" and \"guide mode\". \"insta review\" let you review the visualization of gaze when viewing the last image. \"guide mode\" display the annotation (if any) then display the image, this mode is for teaching which we are still working on.","title":"Config"},{"location":"deep_learning/","text":"Deep learning examples In this page, we introduce how to utilize the collected gaze in deep learning tasks. Here we introduce two \"static heatmap\" methods, which means we do not take the order of the gaze into consideration. Human attention guide network attention We implement a simple yet effective deep learning solution for utilizing the guidance from the radiologist\u2019s gaze [3]. We demonstrate that the extra supervision from expert gaze can improve accuracy, robustness and interpretability of the CAD system. You can read our paper and following code for more detail. In short, we force the network to look at area where radiologist looked. There is a very clean and short solution. Let me explain. In most current neural network, there is a GAP followed by a linear layer where a feature map is fed into it. We need to locate the 'feature map', as following we give an example of PyTorch's official ResNet implementation. We save the feature map, and then we get the weight of the linear layer. Then we just torch.nn.functional.conv2d them to get the result. However, usually we need the cam the same resolution as the image thus a interpolation is needed. You can use the following refine_cams function if you need. If the F.conv2d #torch.nn.functional.conv2d seems like a black magic to you, you can read following explaination and refer to https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html for more detail. What we need is actually a element-wise dot product the feature map vector (i.e. a 1xC tensor) to corresponding linear's weight (i.e. a CxnClasses vec). The linear weight have a dim of CxnClasses, so we just need to dot product element-wise. That, is a 1x1 conv. Maybe now you can understand how the following code make sense. Plug it into your network and try! def refine_cams(cam_original, image_shape): if image_shape[0] != cam_original.size(2) or image_shape[1] != cam_original.size(3): cam_original = F.interpolate( cam_original, image_shape, mode=\"bilinear\", align_corners=True ) B, C, H, W = cam_original.size() cams = [] for idx in range(C): cam = cam_original[:, idx, :, :] cam = cam.view(B, -1) cam_min = cam.min(dim=1, keepdim=True)[0] cam_max = cam.max(dim=1, keepdim=True)[0] norm = cam_max - cam_min norm[norm == 0] = 1e-5 cam = (cam - cam_min) / norm cam = cam.view(B, H, W).unsqueeze(1) cams.append(cam) cams = torch.cat(cams, dim=1) sigmoid_cams = torch.sigmoid(100*(cams - 0.4)) return cams, sigmoid_cams def _forward_impl(self, x): # See note [TorchScript super()] x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) fmap = self.layer4(x) x = self.avgpool(fmap) x = torch.flatten(x, 1) x = self.fc(x) weight = self.fc.weight cam = F.conv2d(fmap, weight.detach().unsqueeze(2).unsqueeze(3), bias=None) cams, sigmoid_cams = refine_cams(cam, self.image_shape) return x, cams, sigmoid_cams Multi-task Karargyris et al.[1] offers a multi-task framework to utlize eye gaze information as the following figure demonstrated. The backbone have two task: 1. classfication and 2. predict human visual attention. Similiar framework can also be found at [2] where they use mouse instead of eye gaze. Reference A. Karargyris, S. Kashyap, I. Lourentzou, J. T. Wu, A. Sharma, M. Tong, S. Abedin,D. Beymer, V. Mukherjee, E. A. Krupinskiet al., \u201cCreation and validation of a chest x-ray dataset with eye-tracking and report dictation for ai development,\u201dScientific data, vol. 8,no. 1, pp. 1\u201318, 2021. L.Li,M.Xu,X.Wang,L.Jiang,andH.Liu,\u201cAttentionbasedglaucoma detection: A large-scale database and cnn model,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 10 571\u201310 580. Wang, Sheng, Xi Ouyang, Tianming Liu, Qian Wang, and Dinggang Shen. \"Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis.\" IEEE Transactions on Medical Imaging (2022).","title":"Medical image supervised learning"},{"location":"deep_learning/#deep-learning-examples","text":"In this page, we introduce how to utilize the collected gaze in deep learning tasks. Here we introduce two \"static heatmap\" methods, which means we do not take the order of the gaze into consideration.","title":"Deep learning examples"},{"location":"deep_learning/#human-attention-guide-network-attention","text":"We implement a simple yet effective deep learning solution for utilizing the guidance from the radiologist\u2019s gaze [3]. We demonstrate that the extra supervision from expert gaze can improve accuracy, robustness and interpretability of the CAD system. You can read our paper and following code for more detail. In short, we force the network to look at area where radiologist looked. There is a very clean and short solution. Let me explain. In most current neural network, there is a GAP followed by a linear layer where a feature map is fed into it. We need to locate the 'feature map', as following we give an example of PyTorch's official ResNet implementation. We save the feature map, and then we get the weight of the linear layer. Then we just torch.nn.functional.conv2d them to get the result. However, usually we need the cam the same resolution as the image thus a interpolation is needed. You can use the following refine_cams function if you need. If the F.conv2d #torch.nn.functional.conv2d seems like a black magic to you, you can read following explaination and refer to https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html for more detail. What we need is actually a element-wise dot product the feature map vector (i.e. a 1xC tensor) to corresponding linear's weight (i.e. a CxnClasses vec). The linear weight have a dim of CxnClasses, so we just need to dot product element-wise. That, is a 1x1 conv. Maybe now you can understand how the following code make sense. Plug it into your network and try! def refine_cams(cam_original, image_shape): if image_shape[0] != cam_original.size(2) or image_shape[1] != cam_original.size(3): cam_original = F.interpolate( cam_original, image_shape, mode=\"bilinear\", align_corners=True ) B, C, H, W = cam_original.size() cams = [] for idx in range(C): cam = cam_original[:, idx, :, :] cam = cam.view(B, -1) cam_min = cam.min(dim=1, keepdim=True)[0] cam_max = cam.max(dim=1, keepdim=True)[0] norm = cam_max - cam_min norm[norm == 0] = 1e-5 cam = (cam - cam_min) / norm cam = cam.view(B, H, W).unsqueeze(1) cams.append(cam) cams = torch.cat(cams, dim=1) sigmoid_cams = torch.sigmoid(100*(cams - 0.4)) return cams, sigmoid_cams def _forward_impl(self, x): # See note [TorchScript super()] x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) fmap = self.layer4(x) x = self.avgpool(fmap) x = torch.flatten(x, 1) x = self.fc(x) weight = self.fc.weight cam = F.conv2d(fmap, weight.detach().unsqueeze(2).unsqueeze(3), bias=None) cams, sigmoid_cams = refine_cams(cam, self.image_shape) return x, cams, sigmoid_cams","title":"Human attention guide network attention"},{"location":"deep_learning/#multi-task","text":"Karargyris et al.[1] offers a multi-task framework to utlize eye gaze information as the following figure demonstrated. The backbone have two task: 1. classfication and 2. predict human visual attention. Similiar framework can also be found at [2] where they use mouse instead of eye gaze.","title":"Multi-task"},{"location":"deep_learning/#reference","text":"A. Karargyris, S. Kashyap, I. Lourentzou, J. T. Wu, A. Sharma, M. Tong, S. Abedin,D. Beymer, V. Mukherjee, E. A. Krupinskiet al., \u201cCreation and validation of a chest x-ray dataset with eye-tracking and report dictation for ai development,\u201dScientific data, vol. 8,no. 1, pp. 1\u201318, 2021. L.Li,M.Xu,X.Wang,L.Jiang,andH.Liu,\u201cAttentionbasedglaucoma detection: A large-scale database and cnn model,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 10 571\u201310 580. Wang, Sheng, Xi Ouyang, Tianming Liu, Qian Wang, and Dinggang Shen. \"Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis.\" IEEE Transactions on Medical Imaging (2022).","title":"Reference"},{"location":"deep_learning_example1/","text":"Great computer vision paper must have a cat in it Task and data At the start of this project (around May 2020), I start with Kaggle's Cat vs Dog to validate the basic idea. Basically, it's a binary classification task to tell an image is cat or dog. And we collected a lot of eye gaze from our lab members. The collected data is then used to train a network. This exploration did not reach the pushlish quality, but I believe it is worth to mention. Result The image below is trained with 1000 training images. Gotta love these cute little guys And we try more class labels, then we found: for the network attetion (CAM), more label=less label+gaze . You may notice we did not report the classification accuracy, because this problem is so easy for these networks, 95%+ acc can be easily achieved.","title":"Natural image DL example"},{"location":"deep_learning_example1/#great-computer-vision-paper-must-have-a-cat-in-it","text":"","title":"Great computer vision paper must have a cat in it"},{"location":"deep_learning_example1/#task-and-data","text":"At the start of this project (around May 2020), I start with Kaggle's Cat vs Dog to validate the basic idea. Basically, it's a binary classification task to tell an image is cat or dog. And we collected a lot of eye gaze from our lab members. The collected data is then used to train a network. This exploration did not reach the pushlish quality, but I believe it is worth to mention.","title":"Task and data"},{"location":"deep_learning_example1/#result","text":"The image below is trained with 1000 training images. Gotta love these cute little guys And we try more class labels, then we found: for the network attetion (CAM), more label=less label+gaze . You may notice we did not report the classification accuracy, because this problem is so easy for these networks, 95%+ acc can be easily achieved.","title":"Result"},{"location":"future/","text":"Mammography & Eye-tracking We are currently working on more image modalities: mammography, Chest X-Ray (Check out CXR project by Karargyris et al. as well: CXR Eye Gaze .) and 3D CT and MR images. Here is a demo on mammography, which demostrate the ability to use zoom and drag a DICOM image: here are the gaze result for this video: About This work is done by Chong Ma, Jiaming Xie and me at ShanghaiTech IDEA Lab. The volunteer in this video is Qiuyi Fu from Ruijin Hospital.","title":"Future Plan"},{"location":"future/#mammography-eye-tracking","text":"We are currently working on more image modalities: mammography, Chest X-Ray (Check out CXR project by Karargyris et al. as well: CXR Eye Gaze .) and 3D CT and MR images. Here is a demo on mammography, which demostrate the ability to use zoom and drag a DICOM image: here are the gaze result for this video:","title":"Mammography &amp; Eye-tracking"},{"location":"future/#about","text":"This work is done by Chong Ma, Jiaming Xie and me at ShanghaiTech IDEA Lab. The volunteer in this video is Qiuyi Fu from Ruijin Hospital.","title":"About"},{"location":"installation/","text":"Installation Plug in your eye-tracker, and set it up. Long story short If you are an experienced programmer and know what you are doing, just install a win-32 python and git clone. Or you can find more detail below. set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 conda activate miceye git clone https://github.com/JamesQFreeman/MICEYE.git pip install python-opencv PyQt5 numpy pillow cd MICEYE python MicEye.py System Since Tobii's sdk did not compile on Linux or MacOS, we use Windows 10 . You can read tobii's doc for more details. In this project, we just use the PyEyetracker I wrote. If you want more than gaze location, see PyEyetracker and write your own. Python Environment It is because the tobii_stream_engine.dll is compiled in 32-bit instead of AMD64, we have to use 32-bit python. If you got conda, it shouldn't be too hard: set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 If you haven't got conda, just download an anaconda or a miniconda. Dependency All the image processing is opencv/numpy style and all the GUI is wrote in PyQt5, so you need to install these as well pip install python-opencv PyQt5 numpy pillow Download MicEye You can git clone or download the zip from Github git clone https://github.com/JamesQFreeman/MICEYE.git Then you are good to go!","title":"Installation"},{"location":"installation/#installation","text":"Plug in your eye-tracker, and set it up.","title":"Installation"},{"location":"installation/#long-story-short","text":"If you are an experienced programmer and know what you are doing, just install a win-32 python and git clone. Or you can find more detail below. set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 conda activate miceye git clone https://github.com/JamesQFreeman/MICEYE.git pip install python-opencv PyQt5 numpy pillow cd MICEYE python MicEye.py","title":"Long story short"},{"location":"installation/#system","text":"Since Tobii's sdk did not compile on Linux or MacOS, we use Windows 10 . You can read tobii's doc for more details. In this project, we just use the PyEyetracker I wrote. If you want more than gaze location, see PyEyetracker and write your own.","title":"System"},{"location":"installation/#python-environment","text":"It is because the tobii_stream_engine.dll is compiled in 32-bit instead of AMD64, we have to use 32-bit python. If you got conda, it shouldn't be too hard: set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 If you haven't got conda, just download an anaconda or a miniconda.","title":"Python Environment"},{"location":"installation/#dependency","text":"All the image processing is opencv/numpy style and all the GUI is wrote in PyQt5, so you need to install these as well pip install python-opencv PyQt5 numpy pillow","title":"Dependency"},{"location":"installation/#download-miceye","text":"You can git clone or download the zip from Github git clone https://github.com/JamesQFreeman/MICEYE.git Then you are good to go!","title":"Download MicEye"},{"location":"post_process/","text":"Post process Find fixation with Levy Flight Finding fixation is not a very simple task. Specially when there are some inter-obersver variance. We provide some solutions (we introduced one here, other methods can be found in code) that worked on our project. But we are not expert at this by any means. We calculate attention level using all points' stepsizes in a time window to calculate Levy Flight distribution's parameter. def levyDistribution(x, a): return a/(x*x) def findLevyPara(stepsizes): res = np.histogram(stepsizes, bins=np.linspace(0, 200, 100), range=(0, 200), density=True) prob = res[0] stepsize = res[1][1:] popt, _ = curve_fit(levyDistribution, stepsize, prob) return popt[0] To heatmap Just \"gaussian\" it: def pointToHeatmap(pointList, gaussianSize=99, normalize=True, heatmapShape=(900, 900), offset=(0,0)): canvas = np.zeros(heatmapShape) for p in pointList: if p[1] < heatmapShape[0] and p[0] < heatmapShape[1]: canvas[p[1]+offset[1]][p[0]+offset[0]] = 1 g = cv.GaussianBlur(canvas, (gaussianSize, gaussianSize), 0, 0) if normalize: g = cv.normalize(g, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F) return g","title":"Post process"},{"location":"post_process/#post-process","text":"","title":"Post process"},{"location":"post_process/#find-fixation-with-levy-flight","text":"Finding fixation is not a very simple task. Specially when there are some inter-obersver variance. We provide some solutions (we introduced one here, other methods can be found in code) that worked on our project. But we are not expert at this by any means. We calculate attention level using all points' stepsizes in a time window to calculate Levy Flight distribution's parameter. def levyDistribution(x, a): return a/(x*x) def findLevyPara(stepsizes): res = np.histogram(stepsizes, bins=np.linspace(0, 200, 100), range=(0, 200), density=True) prob = res[0] stepsize = res[1][1:] popt, _ = curve_fit(levyDistribution, stepsize, prob) return popt[0]","title":"Find fixation with Levy Flight"},{"location":"post_process/#to-heatmap","text":"Just \"gaussian\" it: def pointToHeatmap(pointList, gaussianSize=99, normalize=True, heatmapShape=(900, 900), offset=(0,0)): canvas = np.zeros(heatmapShape) for p in pointList: if p[1] < heatmapShape[0] and p[0] < heatmapShape[1]: canvas[p[1]+offset[1]][p[0]+offset[0]] = 1 g = cv.GaussianBlur(canvas, (gaussianSize, gaussianSize), 0, 0) if normalize: g = cv.normalize(g, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F) return g","title":"To heatmap"}]}