{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Record radiologists' eye gaze when they are labeling images. Why use MicEye Versatile As a label tool, we support class label (keyboard typing) bounding boxes (mouse draw) keypoint label (use eye gaze) During aforemention annotation process, all eye movement will be recorded. Cheaper Start your research using less than 200 bucks! Comparing to other existing software, we use cheaper and more easily obtainable hardware, i.e., Tobii Eye Tracker 4C and Tobii Eye Tracker 5. Easier Here we domonstrate some use cases, you can use it on the go with a laptop or use it on your desk monitor. It should be handful if you want to take it with you to collect data. Credit to Tobii's consumer product lineup, they did pack some really sick hardware in a small form factor. And the code is easy to modify since it is PyQt. You can try it for fun! I understand there are some better eye tracking software out there. However, they are too expensive and hard to use for someone who are trying for fun. And I do believe trying for fun is great, that's why I spent time to organize the code, add comments and docs even I know more thorough literature can be found. At beginning it was not a very serious idea during the pandemic lockdown to combine eye-tracking and medical image analysis (which is my PhD project). After some google search, I found the Tobii Eye-tracker are surprisingly affordable, so I bought it with my own money (my prof paid it soon after). There're no easy-to-use API so at the beginning I record the screen with a red bubble and post-process the video. And we started with the showing cat/dog using PPT so every one in the lab are interesting to take part in. Then I wrap the c++ sdk in python and start to writing the program which become the MicEye now. Linus Torvalds said he built Linux \"just for fun\". Hopefully, if you find this eye tracking things fun, MicEye could help you start something big. A demo We domonstrate an example of radiologist reading knee X-Ray images. Of cause, you could use it in natural image. This red bubble is for demonstration, do not exist in real software. In the future There will be a 3D version. There will be a version that support zoom and drag (coming very soon). Contact me Github issue or wsheng@sjtu.edu.cn","title":"Getting started"},{"location":"#_1","text":"Record radiologists' eye gaze when they are labeling images.","title":""},{"location":"#why-use-miceye","text":"","title":"Why use MicEye"},{"location":"#versatile","text":"As a label tool, we support class label (keyboard typing) bounding boxes (mouse draw) keypoint label (use eye gaze) During aforemention annotation process, all eye movement will be recorded.","title":"Versatile"},{"location":"#cheaper","text":"Start your research using less than 200 bucks! Comparing to other existing software, we use cheaper and more easily obtainable hardware, i.e., Tobii Eye Tracker 4C and Tobii Eye Tracker 5.","title":"Cheaper"},{"location":"#easier","text":"Here we domonstrate some use cases, you can use it on the go with a laptop or use it on your desk monitor. It should be handful if you want to take it with you to collect data. Credit to Tobii's consumer product lineup, they did pack some really sick hardware in a small form factor. And the code is easy to modify since it is PyQt.","title":"Easier"},{"location":"#you-can-try-it-for-fun","text":"I understand there are some better eye tracking software out there. However, they are too expensive and hard to use for someone who are trying for fun. And I do believe trying for fun is great, that's why I spent time to organize the code, add comments and docs even I know more thorough literature can be found. At beginning it was not a very serious idea during the pandemic lockdown to combine eye-tracking and medical image analysis (which is my PhD project). After some google search, I found the Tobii Eye-tracker are surprisingly affordable, so I bought it with my own money (my prof paid it soon after). There're no easy-to-use API so at the beginning I record the screen with a red bubble and post-process the video. And we started with the showing cat/dog using PPT so every one in the lab are interesting to take part in. Then I wrap the c++ sdk in python and start to writing the program which become the MicEye now. Linus Torvalds said he built Linux \"just for fun\". Hopefully, if you find this eye tracking things fun, MicEye could help you start something big.","title":"You can try it for fun!"},{"location":"#a-demo","text":"We domonstrate an example of radiologist reading knee X-Ray images. Of cause, you could use it in natural image. This red bubble is for demonstration, do not exist in real software.","title":"A demo"},{"location":"#in-the-future","text":"There will be a 3D version. There will be a version that support zoom and drag (coming very soon).","title":"In the future"},{"location":"#contact-me","text":"Github issue or wsheng@sjtu.edu.cn","title":"Contact me"},{"location":"about/","text":"Acknowledgement This software is developed by Wang Sheng at Shanghai Jiao Tong University & ShanghaiTech University, supported by Xi Ouyang, Qian Wang and Dinggang Shen. Feel free to check out our labs' websites SJTU MIC Lab and IDEA Lab ShanghaiTech . Thanks for the help from Qingyuan Zhen and Tianming Liu (Univerisity of Georgia), they help revise the paper. And many thanks to the participated radiologist volunteers: Jingyu Zhong and Liping Si. If you find this work interesting, check out this project by Karargyris et al. as well: CXR Eye Gaze .","title":"About"},{"location":"about/#acknowledgement","text":"This software is developed by Wang Sheng at Shanghai Jiao Tong University & ShanghaiTech University, supported by Xi Ouyang, Qian Wang and Dinggang Shen. Feel free to check out our labs' websites SJTU MIC Lab and IDEA Lab ShanghaiTech . Thanks for the help from Qingyuan Zhen and Tianming Liu (Univerisity of Georgia), they help revise the paper. And many thanks to the participated radiologist volunteers: Jingyu Zhong and Liping Si. If you find this work interesting, check out this project by Karargyris et al. as well: CXR Eye Gaze .","title":"Acknowledgement"},{"location":"data_collection/","text":"Collect Data Calibrate Use Tobii's software to do the calibration. I did try to write my own calibration but then I found Tobii's is great which even had a game to play (Now I can't find the game, seems like it is deleted). After the calibration, you can open MicEye by python miceye.py Start collection Place all the images in a same folder, then change the \"image folder\" option in config.json . The data collection begin with typing the name down. And a message is shown to the volunteers to tell them how the experiment is setted up. You can modify the message at main.py . Simply look Just press the \"Enter\" for the next image. Gaze will be recorded. Type to label This is for quantification/classification. For example, in knee X-Ray image, there is a quantification criteria named KL-Grade, range from 0 to 4, 0 means healthy and 4 is very ill. If do cat/dog classification, type 0 for cat and 1 for dog. It's on you. Draw bounding boxes You can add bounding boxes by click \"Add Bounding Boxes\", then draw some boxes. Laser eye mode It is a keypoint label, look at where you want label, and press \"L\" key. Save data The gaze data is saved automatically after the experiment is finished. In default setting, the data will be saved to logs/name year-month-day-hour-minute.csv . In the save file include multiple lines, each line include information seperated by semicolon, such as: some-dir/9063823L.png;1;[[206, 412], [206, 412], [205, 411], ..., [105, 501]];[];(-1, -1) It is orgnized by image-location;class-label;[gaze-loc-1,gaze-loc-2,...,gaze-loc-n];[bbox_x1,bbox_y1,bbox_x2,bbox_y2];keypoint-loc in which all the coordenate is the image coordinate system. For example, upper left of the image is [0,0] . Config We also provide config.json , so you can modify it. { \"image folder\": \"image folder\", \"save log to\": \"./logs\", \"random display order\": true, \"image height\": 900, \"loading wait\": 3, \"font\": \"Helvetica\", \"dark mode\": true, \"insta review\": false, \"guide mode\": true } Other options should be pretty straight forward. Let me explain \"insta review\" and \"guide mode\". \"insta review\" let you review the visualization of gaze when viewing the last image. \"guide mode\" display the annotation (if any) then display the image, this mode is for teaching which we are still working on.","title":"Data collection"},{"location":"data_collection/#collect-data","text":"","title":"Collect Data"},{"location":"data_collection/#calibrate","text":"Use Tobii's software to do the calibration. I did try to write my own calibration but then I found Tobii's is great which even had a game to play (Now I can't find the game, seems like it is deleted). After the calibration, you can open MicEye by python miceye.py","title":"Calibrate"},{"location":"data_collection/#start-collection","text":"Place all the images in a same folder, then change the \"image folder\" option in config.json . The data collection begin with typing the name down. And a message is shown to the volunteers to tell them how the experiment is setted up. You can modify the message at main.py .","title":"Start collection"},{"location":"data_collection/#simply-look","text":"Just press the \"Enter\" for the next image. Gaze will be recorded.","title":"Simply look"},{"location":"data_collection/#type-to-label","text":"This is for quantification/classification. For example, in knee X-Ray image, there is a quantification criteria named KL-Grade, range from 0 to 4, 0 means healthy and 4 is very ill. If do cat/dog classification, type 0 for cat and 1 for dog. It's on you.","title":"Type to label"},{"location":"data_collection/#draw-bounding-boxes","text":"You can add bounding boxes by click \"Add Bounding Boxes\", then draw some boxes.","title":"Draw bounding boxes"},{"location":"data_collection/#laser-eye-mode","text":"It is a keypoint label, look at where you want label, and press \"L\" key.","title":"Laser eye mode"},{"location":"data_collection/#save-data","text":"The gaze data is saved automatically after the experiment is finished. In default setting, the data will be saved to logs/name year-month-day-hour-minute.csv . In the save file include multiple lines, each line include information seperated by semicolon, such as: some-dir/9063823L.png;1;[[206, 412], [206, 412], [205, 411], ..., [105, 501]];[];(-1, -1) It is orgnized by image-location;class-label;[gaze-loc-1,gaze-loc-2,...,gaze-loc-n];[bbox_x1,bbox_y1,bbox_x2,bbox_y2];keypoint-loc in which all the coordenate is the image coordinate system. For example, upper left of the image is [0,0] .","title":"Save data"},{"location":"data_collection/#config","text":"We also provide config.json , so you can modify it. { \"image folder\": \"image folder\", \"save log to\": \"./logs\", \"random display order\": true, \"image height\": 900, \"loading wait\": 3, \"font\": \"Helvetica\", \"dark mode\": true, \"insta review\": false, \"guide mode\": true } Other options should be pretty straight forward. Let me explain \"insta review\" and \"guide mode\". \"insta review\" let you review the visualization of gaze when viewing the last image. \"guide mode\" display the annotation (if any) then display the image, this mode is for teaching which we are still working on.","title":"Config"},{"location":"deep_learning/","text":"Deep learning examples In this page, we introduce how to utilize the collected gaze in deep learning tasks. Here we introduce two \"static heatmap\" methods, which means we do not take the order of the gaze into consideration. Human attention guide network attention We implement a simple yet effective deep learning solution for utilizing the guidance from the radiologist\u2019s gaze. We demonstrate that the extra supervision from expert gaze can improve accuracy, robustness and interpretability of the CAD system. You can read our paper and code for more detail. In short, we force the network to look at area where radiologist looked. Multi-task Karargyris et al.[1] offers a multi-task framework to utlize eye gaze information as the following figure demonstrated. The backbone have two task: 1. classfication and 2. predict human visual attention. Similiar framework can also be found at [2] where they use mouse instead of eye gaze. Reference A. Karargyris, S. Kashyap, I. Lourentzou, J. T. Wu, A. Sharma, M. Tong, S. Abedin,D. Beymer, V. Mukherjee, E. A. Krupinskiet al., \u201cCreation and validation of a chest x-ray dataset with eye-tracking and report dictation for ai development,\u201dScientific data, vol. 8,no. 1, pp. 1\u201318, 2021. L.Li,M.Xu,X.Wang,L.Jiang,andH.Liu,\u201cAttentionbasedglaucoma detection: A large-scale database and cnn model,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 10 571\u201310 580.","title":"Medical image DL example"},{"location":"deep_learning/#deep-learning-examples","text":"In this page, we introduce how to utilize the collected gaze in deep learning tasks. Here we introduce two \"static heatmap\" methods, which means we do not take the order of the gaze into consideration.","title":"Deep learning examples"},{"location":"deep_learning/#human-attention-guide-network-attention","text":"We implement a simple yet effective deep learning solution for utilizing the guidance from the radiologist\u2019s gaze. We demonstrate that the extra supervision from expert gaze can improve accuracy, robustness and interpretability of the CAD system. You can read our paper and code for more detail. In short, we force the network to look at area where radiologist looked.","title":"Human attention guide network attention"},{"location":"deep_learning/#multi-task","text":"Karargyris et al.[1] offers a multi-task framework to utlize eye gaze information as the following figure demonstrated. The backbone have two task: 1. classfication and 2. predict human visual attention. Similiar framework can also be found at [2] where they use mouse instead of eye gaze.","title":"Multi-task"},{"location":"deep_learning/#reference","text":"A. Karargyris, S. Kashyap, I. Lourentzou, J. T. Wu, A. Sharma, M. Tong, S. Abedin,D. Beymer, V. Mukherjee, E. A. Krupinskiet al., \u201cCreation and validation of a chest x-ray dataset with eye-tracking and report dictation for ai development,\u201dScientific data, vol. 8,no. 1, pp. 1\u201318, 2021. L.Li,M.Xu,X.Wang,L.Jiang,andH.Liu,\u201cAttentionbasedglaucoma detection: A large-scale database and cnn model,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 10 571\u201310 580.","title":"Reference"},{"location":"deep_learning_example1/","text":"Great computer vision paper must have a cat in it Task and data At the start of this project (around May 2020), I start with Kaggle's Cat vs Dog to validate the basic idea. Basically, it's a binary classification task to tell an image is cat or dog. And we collected a lot of eye gaze from our lab members. The collected data is then used to train a network. This exploration did not reach the pushlish quality, but I believe it is worth to mention. Result The image below is trained with 1000 training images. Gotta love these cute little guys And we try more class labels, then we found: for the network attetion (CAM), more label=less label+gaze . You may notice we did not report the classification accuracy, because this problem is so easy for these networks, 95%+ acc can be easily achieved.","title":"Natural image DL example"},{"location":"deep_learning_example1/#great-computer-vision-paper-must-have-a-cat-in-it","text":"","title":"Great computer vision paper must have a cat in it"},{"location":"deep_learning_example1/#task-and-data","text":"At the start of this project (around May 2020), I start with Kaggle's Cat vs Dog to validate the basic idea. Basically, it's a binary classification task to tell an image is cat or dog. And we collected a lot of eye gaze from our lab members. The collected data is then used to train a network. This exploration did not reach the pushlish quality, but I believe it is worth to mention.","title":"Task and data"},{"location":"deep_learning_example1/#result","text":"The image below is trained with 1000 training images. Gotta love these cute little guys And we try more class labels, then we found: for the network attetion (CAM), more label=less label+gaze . You may notice we did not report the classification accuracy, because this problem is so easy for these networks, 95%+ acc can be easily achieved.","title":"Result"},{"location":"installation/","text":"Installation Plug in your eye-tracker, and set it up. Long story short If you are an experienced programmer and know what you are doing, just install a win-32 python and git clone. Or you can find more detail below. set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 conda activate miceye git clone https://github.com/JamesQFreeman/MICEYE.git pip install python-opencv PyQt5 numpy pillow cd MICEYE python MicEye.py System Since Tobii's sdk did not compile on Linux or MacOS, we use Windows 10 . You can read tobii's doc for more details. In this project, we just use the PyEyetracker I wrote. If you want more than gaze location, see PyEyetracker and write your own. Python Environment It is because the tobii_stream_engine.dll is compiled in 32-bit instead of AMD64, we have to use 32-bit python. If you got conda, it shouldn't be too hard: set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 If you haven't got conda, just download an anaconda or a miniconda. Dependency All the image processing is opencv/numpy style and all the GUI is wrote in PyQt5, so you need to install these as well pip install python-opencv PyQt5 numpy pillow Download MicEye You can git clone or download the zip from Github git clone https://github.com/JamesQFreeman/MICEYE.git Then you are good to go!","title":"Installation"},{"location":"installation/#installation","text":"Plug in your eye-tracker, and set it up.","title":"Installation"},{"location":"installation/#long-story-short","text":"If you are an experienced programmer and know what you are doing, just install a win-32 python and git clone. Or you can find more detail below. set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 conda activate miceye git clone https://github.com/JamesQFreeman/MICEYE.git pip install python-opencv PyQt5 numpy pillow cd MICEYE python MicEye.py","title":"Long story short"},{"location":"installation/#system","text":"Since Tobii's sdk did not compile on Linux or MacOS, we use Windows 10 . You can read tobii's doc for more details. In this project, we just use the PyEyetracker I wrote. If you want more than gaze location, see PyEyetracker and write your own.","title":"System"},{"location":"installation/#python-environment","text":"It is because the tobii_stream_engine.dll is compiled in 32-bit instead of AMD64, we have to use 32-bit python. If you got conda, it shouldn't be too hard: set CONDA_SUBDIR=win-32 conda create -n miceye python=3.7 If you haven't got conda, just download an anaconda or a miniconda.","title":"Python Environment"},{"location":"installation/#dependency","text":"All the image processing is opencv/numpy style and all the GUI is wrote in PyQt5, so you need to install these as well pip install python-opencv PyQt5 numpy pillow","title":"Dependency"},{"location":"installation/#download-miceye","text":"You can git clone or download the zip from Github git clone https://github.com/JamesQFreeman/MICEYE.git Then you are good to go!","title":"Download MicEye"},{"location":"post_process/","text":"Post process Find fixation with Levy Flight Finding fixation is not a very simple task. Specially when there are some inter-obersver variance. We provide some solutions (we introduced one here, other methods can be found in code) that worked on our project. But we are not expert at this by any means. We calculate attention level using all points' stepsizes in a time window to calculate Levy Flight distribution's parameter. def levyDistribution(x, a): return a/(x*x) def findLevyPara(stepsizes): res = np.histogram(stepsizes, bins=np.linspace(0, 200, 100), range=(0, 200), density=True) prob = res[0] stepsize = res[1][1:] popt, _ = curve_fit(levyDistribution, stepsize, prob) return popt[0] To heatmap Just \"gaussian\" it: def pointToHeatmap(pointList, gaussianSize=99, normalize=True, heatmapShape=(900, 900), offset=(0,0)): canvas = np.zeros(heatmapShape) for p in pointList: if p[1] < heatmapShape[0] and p[0] < heatmapShape[1]: canvas[p[1]+offset[1]][p[0]+offset[0]] = 1 g = cv.GaussianBlur(canvas, (gaussianSize, gaussianSize), 0, 0) if normalize: g = cv.normalize(g, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F) return g","title":"Post process"},{"location":"post_process/#post-process","text":"","title":"Post process"},{"location":"post_process/#find-fixation-with-levy-flight","text":"Finding fixation is not a very simple task. Specially when there are some inter-obersver variance. We provide some solutions (we introduced one here, other methods can be found in code) that worked on our project. But we are not expert at this by any means. We calculate attention level using all points' stepsizes in a time window to calculate Levy Flight distribution's parameter. def levyDistribution(x, a): return a/(x*x) def findLevyPara(stepsizes): res = np.histogram(stepsizes, bins=np.linspace(0, 200, 100), range=(0, 200), density=True) prob = res[0] stepsize = res[1][1:] popt, _ = curve_fit(levyDistribution, stepsize, prob) return popt[0]","title":"Find fixation with Levy Flight"},{"location":"post_process/#to-heatmap","text":"Just \"gaussian\" it: def pointToHeatmap(pointList, gaussianSize=99, normalize=True, heatmapShape=(900, 900), offset=(0,0)): canvas = np.zeros(heatmapShape) for p in pointList: if p[1] < heatmapShape[0] and p[0] < heatmapShape[1]: canvas[p[1]+offset[1]][p[0]+offset[0]] = 1 g = cv.GaussianBlur(canvas, (gaussianSize, gaussianSize), 0, 0) if normalize: g = cv.normalize(g, None, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F) return g","title":"To heatmap"}]}